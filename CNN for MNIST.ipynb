{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size, batch size is how much data you feed for training in one iteration\n",
    "batch_size_train = 64 # We use a small batch size here for training\n",
    "batch_size_test = 1024 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define how image transformed\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataloader, in PyTorch, we feed the trainer data with use of dataloader\n",
    "## We create dataloader with dataset from torchvision, \n",
    "## and we dont have to download it seperately, all automatically done\n",
    "\n",
    "#image datasets\n",
    "train_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "test_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                          train=False, \n",
    "                                          download=True,\n",
    "                                          transform=image_transform)\n",
    "#data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size_test, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See an example image\n",
    "\n",
    "_, (example_datas, labels) = next(enumerate(test_loader))\n",
    "sample = example_datas[0][0]\n",
    "# show the data\n",
    "plt.imshow(sample, cmap='gray', interpolation='none')\n",
    "print(\"Label: \"+ str(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #input channel 1, output channel 10\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, stride=1)\n",
    "        #input channel 10, output channel 20\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1)\n",
    "        #dropout layer\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        #fully connected layer\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_drop(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model and optimizer\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "device = \"cpu\"\n",
    "model = CNN().to(device) #using cpu here\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define train function\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10000):\n",
    "    model.train()\n",
    "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    counter = 0\n",
    "    for batch_idx, (data, target) in enumerate(tk0):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        tk0.set_postfix(loss=(loss.item()*data.size(0) / (counter * train_loader.batch_size)))\n",
    "        \n",
    "    correct = 0    \n",
    "    for batch_idx, (data, target) in enumerate(tk0):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "    print(f'\\nTrain set: Accuracy {correct/len(train_loader.dataset)}')\n",
    "\n",
    "##define test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8283/4290178834.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tk0 = tqdm(train_loader, total=int(len(train_loader)))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b821fce63b4e588227a300b28cd7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8283/5768528.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Accuracy 0.8559333333333333\n",
      "\n",
      "Test set: Average loss: 0.3340, Accuracy: 9004/10000 (90%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ac60e551a4466e9c45e427b246732c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Accuracy 0.8962666666666667\n",
      "\n",
      "Test set: Average loss: 0.2262, Accuracy: 9314/10000 (93%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74407aaf166f458abe94c5d65c34ae82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Accuracy 0.9143333333333333\n",
      "\n",
      "Test set: Average loss: 0.1883, Accuracy: 9451/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2             [-1, 20, 8, 8]           5,020\n",
      "         Dropout2d-3             [-1, 20, 8, 8]               0\n",
      "            Linear-4                   [-1, 50]          16,050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.15\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8283/5768528.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 28, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "cs231n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
