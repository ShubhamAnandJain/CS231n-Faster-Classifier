{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1653204886178,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "bFB6WDmimTpS",
    "outputId": "672d2a42-ea4c-43f3-e1ca-1367c2868357"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# FOLDERNAME = 'CS231n_project/'\n",
    "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4353,
     "status": "ok",
     "timestamp": 1653204890527,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "ejjKJYC2YffB",
    "outputId": "7a1aad97-7fc6-4c21-fa04-24f4508284e0"
   },
   "outputs": [],
   "source": [
    "# !pip install torch==1.7 torchvision==0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6003,
     "status": "ok",
     "timestamp": 1653204896527,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "TzJV-LLVhBt5",
    "outputId": "03468936-f099-4c8f-935b-9dfc257e456a"
   },
   "outputs": [],
   "source": [
    "# %cd approx/src/pytorch/cpp\n",
    "# !python setup.py install\n",
    "# %cd ../../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1653204897473,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "uGvmWqNZD2hh",
    "outputId": "fd0ce3ef-78c9-4a72-bef1-a3b94a03a0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from conv_norm import PreConv\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from utils import ImportanceSampler\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1653204897475,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "BMatykiuGnJT"
   },
   "outputs": [],
   "source": [
    "from utils import get_accuracy, load_dataset\n",
    "from models import get_model\n",
    "check_accuracy = lambda loader, model: get_accuracy(loader, model, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1653206021690,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "dLHtcW9UHqOt"
   },
   "outputs": [],
   "source": [
    "def train_model(model_name, dataset_name, model_params={}, hyperparams={}):\n",
    "\n",
    "  learning_rate = hyperparams.get('lr', 1e-3)\n",
    "  num_epochs = hyperparams.get('num_epochs', 10)\n",
    "  weight_decay = hyperparams.get('weight_decay', 0)\n",
    "  train_ratio = hyperparams.get('train_ratio', 0.8)\n",
    "  batch_size = hyperparams.get('batch_size', 64)\n",
    "  seed = hyperparams.get('seed', 0)\n",
    "  imp_sampling = model_params.get('importance_sampling', False)\n",
    "  gamma = model_params.get('gamma', 0.9)\n",
    "  loss_beta = model_params.get('loss_beta', 1)\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  loader_train, loader_val, loader_test, num_train, num_channels = load_dataset(dataset_name, train_ratio, batch_size)\n",
    "  model = get_model(model_name, model_params, learning_rate, loader_train, num_channels, device)\n",
    "\n",
    "  print(\"Model architecture:\")\n",
    "  print(model)\n",
    "\n",
    "  print(f'INFO: Training {model_name} on {dataset_name} with lr {learning_rate}, num_epochs={num_epochs}, weight_decay={weight_decay}')\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "\n",
    "  epoch_vals = []\n",
    "  \n",
    "  weight = torch.tensor([1.0]*num_train)\n",
    "\n",
    "  t_acc, t_loss = check_accuracy(loader_train, model)\n",
    "  val_acc, val_loss = check_accuracy(loader_val, model)\n",
    "  \n",
    "  start = timer()\n",
    "  c_time = timer()-start\n",
    "\n",
    "  running_loss = 0.0\n",
    "  loss_momentum_factor = 0.9\n",
    "  loss_iter_cnt = 1\n",
    "    \n",
    "  print(f'Plot: Train, {0}, {t_loss:.3f}, {t_acc:.2f}, {c_time:.1f}')\n",
    "  print(f'Plot: Val, {0}, {val_loss:.3f}, {val_acc:.2f}, {c_time:.1f}')\n",
    "\n",
    "  for e in range(num_epochs):\n",
    "\n",
    "    doUniform = (e == 0) or (imp_sampling == False)\n",
    "    loader_train_sampled = loader_train\n",
    "    if not doUniform:\n",
    "      train_sampler = ImportanceSampler(num_train, weight, batch_size)\n",
    "      loader_train_sampled, _, _, _, _ = load_dataset(dataset_name, train_ratio, batch_size, train_sampler)\n",
    "    \n",
    "    for t, tpl in enumerate(loader_train_sampled):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()  # put model to training mode\n",
    "        x = tpl[0].to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        y = tpl[1].to(device=device, dtype=torch.long)\n",
    "\n",
    "        scores = model(x)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "        running_loss = loss_momentum_factor * running_loss + (1 - loss_momentum_factor) * loss\n",
    "        final_loss = running_loss / (1 - (loss_momentum_factor ** loss_iter_cnt))\n",
    "        loss_iter_cnt += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not doUniform:\n",
    "          idx = tpl[2]\n",
    "          weight[idx] = gamma * weight[idx] + (1 - gamma) * loss_beta * float(loss/final_loss)\n",
    "\n",
    "    t_acc, t_loss = check_accuracy(loader_train, model)\n",
    "    val_acc, val_loss = check_accuracy(loader_val, model)\n",
    "    c_time = timer()-start\n",
    "\n",
    "    print(f'Plot: Train, {e+1}, {t_loss:.3f}, {t_acc:.2f}, {c_time:.1f}')\n",
    "    print(f'Plot: Val, {e+1}, {val_loss:.3f}, {val_acc:.2f}, {c_time:.1f}')\n",
    "\n",
    "  test_acc, test_loss = check_accuracy(loader_test, model)\n",
    "  print(f'Plot: Test, {val_loss:.3f}, {val_acc:.2f}, {c_time:.1f}')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhJmW7E1cMBF",
    "outputId": "d812c321-f54b-4aaf-c69b-ce76893369c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Model architecture:\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "INFO: Training Resnet18 on CIFAR100 with lr 0.003, num_epochs=15, weight_decay=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3192/3296015221.py\", line 36, in <module>\n",
      "    test_setup()\n",
      "  File \"/tmp/ipykernel_3192/3296015221.py\", line 34, in test_setup\n",
      "    train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
      "  File \"/tmp/ipykernel_3192/1520961306.py\", line 30, in train_model\n",
      "    t_acc, t_loss = check_accuracy(loader_train, model)\n",
      "  File \"/tmp/ipykernel_3192/3748356230.py\", line 3, in <lambda>\n",
      "    check_accuracy = lambda loader, model: get_accuracy(loader, model, device, dtype)\n",
      "  File \"/home/ubuntu/CS231n-Faster-Classifier/src/utils.py\", line 19, in get_accuracy\n",
      "    scores = model(x)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torchvision/models/resnet.py\", line 220, in forward\n",
      "    return self._forward_impl(x)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torchvision/models/resnet.py\", line 203, in _forward_impl\n",
      "    x = self.conv1(x)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 423, in forward\n",
      "    return self._conv_forward(input, self.weight)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 420, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3192/3296015221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtest_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3192/3296015221.py\u001b[0m in \u001b[0;36mtest_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resnet18'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CIFAR100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3192/1520961306.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, dataset_name, model_params, hyperparams)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mt_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3192/3748356230.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheck_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS231n-Faster-Classifier/src/utils.py\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(loader, model, device, dtype)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2064\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# learning_rate = hyperparams.get('lr', 1e-3)\n",
    "# num_epochs = hyperparams.get('num_epochs', 10)\n",
    "# weight_decay = hyperparams.get('weight_decay', 0)\n",
    "# train_ratio = hyperparams.get('train_ratio', 0.8)\n",
    "# batch_size = hyperparams.get('batch_size', 64)\n",
    "# imp_sampling = model_params.get('importance_sampling', False)\n",
    "# gamma = model_params.get('gamma', 0.9)\n",
    "\n",
    "gradinit_params = {\n",
    "    \"gradinit_iters\": 50,\n",
    "}\n",
    "model_params = {\n",
    "    # \"gradinit\": gradinit_params,\n",
    "    # \"convnorm\" : True,\n",
    "    # \"approx_mult\" : 0.2,\n",
    "    \"importance_sampling\" : True,\n",
    "    # \"gamma\" : 0.9\n",
    "}\n",
    "hyperparams = {\n",
    "    \"lr\" : 3e-3,\n",
    "    \"num_epochs\" : 15,\n",
    "    \"weight_decay\" : 0,\n",
    "    \"train_ratio\" : 0.8,\n",
    "    \"batch_size\" : 1024,\n",
    "}\n",
    "\n",
    "# train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
    "# model_params[\"convnorm\"] = True\n",
    "# train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
    "\n",
    "def test_setup():\n",
    "    for gamma in [0.9, 0.99]:\n",
    "        model_params['gamma'] = gamma\n",
    "        train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
    "\n",
    "test_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Model architecture:\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "INFO: Training Resnet18 on CIFAR100 with lr 0.003, num_epochs=15, weight_decay=0\n",
      "Plot: Train, 0, 6.908, 0.00, 0.0\n",
      "Plot: Val, 0, 6.924, 0.00, 0.0\n",
      "Plot: Train, 1, 4.356, 12.51, 56.0\n",
      "Plot: Val, 1, 4.144, 11.41, 56.0\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 2, 3.202, 21.00, 142.4\n",
      "Plot: Val, 2, 3.677, 18.47, 142.4\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 3, 3.056, 28.36, 229.5\n",
      "Plot: Val, 3, 3.351, 23.86, 229.5\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 4, 2.645, 32.95, 318.3\n",
      "Plot: Val, 4, 3.070, 27.10, 318.3\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 5, 2.184, 41.40, 412.8\n",
      "Plot: Val, 5, 2.870, 31.29, 412.8\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 6, 1.891, 47.31, 500.3\n",
      "Plot: Val, 6, 2.577, 33.84, 500.3\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 7, 1.924, 52.34, 590.1\n",
      "Plot: Val, 7, 2.632, 35.10, 590.1\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 8, 1.674, 51.90, 679.6\n",
      "Plot: Val, 8, 2.953, 33.55, 679.6\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 9, 1.484, 57.91, 820.4\n",
      "Plot: Val, 9, 2.901, 34.04, 820.4\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 10, 1.126, 63.13, 960.1\n",
      "Plot: Val, 10, 2.798, 36.29, 960.1\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 11, 1.084, 66.20, 1101.7\n",
      "Plot: Val, 11, 3.147, 35.28, 1101.7\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 12, 0.972, 74.53, 1243.1\n",
      "Plot: Val, 12, 2.944, 36.66, 1243.1\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 13, 0.595, 73.09, 1383.2\n",
      "Plot: Val, 13, 3.456, 36.30, 1383.2\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 14, 0.779, 82.20, 1523.3\n",
      "Plot: Val, 14, 3.064, 37.48, 1523.3\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 15, 0.694, 85.06, 1664.1\n",
      "Plot: Val, 15, 3.314, 37.70, 1664.1\n",
      "Plot: Test, 3.314, 37.70, 1664.1\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Model architecture:\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "INFO: Training Resnet18 on CIFAR100 with lr 0.003, num_epochs=15, weight_decay=0\n",
      "Plot: Train, 0, 6.908, 0.00, 0.0\n",
      "Plot: Val, 0, 6.924, 0.00, 0.0\n",
      "Plot: Train, 1, 4.199, 13.76, 115.5\n",
      "Plot: Val, 1, 4.164, 12.89, 115.5\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 2, 3.167, 21.47, 243.7\n",
      "Plot: Val, 2, 3.633, 18.38, 243.7\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 3, 2.996, 28.93, 371.3\n",
      "Plot: Val, 3, 3.210, 24.18, 371.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 4, 2.371, 34.71, 505.2\n",
      "Plot: Val, 4, 3.049, 27.12, 505.2\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 5, 2.500, 40.36, 648.1\n",
      "Plot: Val, 5, 3.062, 29.90, 648.1\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 6, 2.222, 44.01, 783.2\n",
      "Plot: Val, 6, 3.022, 30.49, 783.2\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 7, 1.560, 52.89, 870.4\n",
      "Plot: Val, 7, 2.926, 33.34, 870.4\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 8, 2.017, 55.32, 957.5\n",
      "Plot: Val, 8, 3.201, 32.22, 957.5\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 9, 1.258, 61.15, 1047.7\n",
      "Plot: Val, 9, 3.026, 34.05, 1047.7\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 10, 1.453, 63.43, 1134.6\n",
      "Plot: Val, 10, 3.168, 33.63, 1134.6\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 11, 0.919, 70.22, 1226.0\n",
      "Plot: Val, 11, 3.187, 34.74, 1226.0\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 12, 0.629, 76.94, 1316.8\n",
      "Plot: Val, 12, 3.062, 36.00, 1316.8\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 13, 0.737, 81.19, 1408.9\n",
      "Plot: Val, 13, 3.217, 37.24, 1408.9\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 14, 0.783, 82.77, 1504.0\n",
      "Plot: Val, 14, 3.298, 36.99, 1504.0\n",
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "Plot: Train, 15, 0.696, 82.92, 1600.6\n",
      "Plot: Val, 15, 3.419, 36.63, 1600.6\n",
      "Plot: Test, 3.419, 36.63, 1600.6\n"
     ]
    }
   ],
   "source": [
    "def test_setup():\n",
    "    for gamma in [0.9]:\n",
    "        for loss_beta in [0.1, 10]:\n",
    "            model_params['gamma'] = gamma\n",
    "            model_params['loss_beta'] = loss_beta\n",
    "            train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
    "\n",
    "test_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_p37",
   "language": "python",
   "name": "pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
