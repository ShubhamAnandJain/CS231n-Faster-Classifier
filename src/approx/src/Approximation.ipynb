{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import csv\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare several algorithms for approximate matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximation algorithm: top-k sampling without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(A,B,K):\n",
    "    # calculate norms of the columns of A and rows of B\n",
    "    a_col_norms = np.linalg.norm(A,axis=0)\n",
    "    b_row_norms = np.linalg.norm(B,axis=1)\n",
    "    \n",
    "    # multiply both norms element-wise to and pick the indices of the top K column-row pairs\n",
    "    norm_mult = np.multiply(a_col_norms,b_row_norms)\n",
    "    top_k_indices = np.sort(np.argsort(norm_mult)[::-1][:K])\n",
    "    \n",
    "    # pick top-k column-row pairs to form new smaller matrices\n",
    "    A_top_k_cols = A[:,top_k_indices]\n",
    "    B_top_k_rows = B[top_k_indices,:]\n",
    "       \n",
    "    # multiply smaller matrices\n",
    "    C_approx = np.dot(A_top_k_cols, B_top_k_rows)\n",
    "    return C_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximation algorith: column-row sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A,B - input matrices\n",
    "    K - number of column-row elements to sample\n",
    "    with_replacement - True means sampling is done with replacement, False means sampling without replacement\n",
    "    optimal_prob - True means sampling probability is proportional to |Ai|*|Bj|. False means random distribution\n",
    "    scale - True means each column-row is scaled by 1/sqrt(K*pi) to ensure bias 0\n",
    "'''\n",
    "def column_row(A,B,K,with_replacement = True, optimal_prob = True, scale=True, debug=False):\n",
    "    # calculate norms of the columns of A and rows of B\n",
    "    a_col_norms = np.linalg.norm(A,axis=0)\n",
    "    b_row_norms = np.linalg.norm(B,axis=1)\n",
    "   \n",
    "    # multiply both norms element-wise\n",
    "    norm_mult = np.multiply(a_col_norms,b_row_norms)\n",
    "    sum_norm_mult = np.sum(norm_mult)\n",
    "    \n",
    "    if optimal_prob == True and sum_norm_mult != 0:\n",
    "        prob_dist = norm_mult/sum_norm_mult\n",
    "    else:\n",
    "        prob_dist = np.ones(A.shape[1])/A.shape[1] # uniform distributionwill be treated as uniform by np.random.choice \n",
    "    \n",
    "    # scale input matrices according to probabilities.\n",
    "    # For convenience we implement it by creating a diagonal matrix and multiplying (other implementations are possible).\n",
    "    if scale == True:\n",
    "        scale_matrix = np.diag(np.divide(1,np.sqrt(np.multiply(K,prob_dist))))\n",
    "    else:\n",
    "        scale_matrix = np.diag(np.ones(A.shape[1]))\n",
    "\n",
    "    A_scaled = np.dot(A,scale_matrix)\n",
    "    B_scaled = np.dot(scale_matrix,B)\n",
    "    \n",
    "    sample_indices = np.random.choice(A.shape[1], size=K, replace=with_replacement, p=prob_dist)\n",
    "    \n",
    "    # sample k column-row pairs to form new smaller matrices\n",
    "    A_k_cols = A_scaled[:,sample_indices]\n",
    "    B_k_rows = B_scaled[sample_indices,:]\n",
    "       \n",
    "    # multiply smaller matrices\n",
    "    C_approx = np.dot(A_k_cols, B_k_rows)\n",
    "\n",
    "    if debug == True:\n",
    "        print ('a_col_norms is ' + str(a_col_norms))\n",
    "        print ('b_row_norms is ' + str(b_row_norms))\n",
    "        print ('norm_mult is ' + str(norm_mult))\n",
    "        print ('sum_norm_mult is ' + str(sum_norm_mult))\n",
    "        print ('prob_dist is ' + str(prob_dist))\n",
    "        print ('scale matrix is ' + str(scale_matrix))\n",
    "        print ('A_scaled is ' + str(A_scaled))\n",
    "        print ('B_scaled is ' + str(B_scaled))\n",
    "        print ('sample_indices are ' + str(sample_indices))\n",
    "        print ('Frobenius error bound is '+ str(sum_norm_mult**2/K -  np.linalg.norm(A.dot(B)/K)))\n",
    "        print ('A_k_cols is ')\n",
    "        print (A_k_cols)\n",
    "        print ('B_k_rows is ')\n",
    "        print (B_k_rows)\n",
    "        print ('C_approx is ')\n",
    "        print (C_approx)\n",
    "    \n",
    "    return C_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximation algorithm - Bernoulli sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A,B - input matrices\n",
    "    K - sampling parameter\n",
    "    scale - True means each column-row is scaled by 1/sqrt(pi) to ensure bias 0\n",
    "'''\n",
    "def column_row_bern(A,B,K, scale=True, debug=False):\n",
    "    # calculate norms of the columns of A and rows of B\n",
    "    a_col_norms = np.linalg.norm(A,axis=0)\n",
    "    b_row_norms = np.linalg.norm(B,axis=1)\n",
    "   \n",
    "    # multiply both norms element-wise\n",
    "    norm_mult = np.multiply(a_col_norms,b_row_norms)\n",
    "    sum_norm_mult = np.sum(norm_mult)\n",
    "    \n",
    "    if sum_norm_mult != 0:\n",
    "        prob_dist = K*norm_mult/sum_norm_mult\n",
    "    else:\n",
    "        prob_dist = np.ones(A.shape[1]) \n",
    "    \n",
    "    prob_dist = np.clip(prob_dist,0,1)\n",
    "    \n",
    "    # scale input matrices according to probabilities.\n",
    "    # For convenience we implement it by creating a diagonal matrix and multiplying (other implementations are possible).\n",
    "    if scale == True:\n",
    "        scale_matrix = np.diag(np.divide(1,np.sqrt(prob_dist)))\n",
    "    else:\n",
    "        scale_matrix = np.diag(np.ones(A.shape[1]))\n",
    "\n",
    "    A_scaled = np.dot(A,scale_matrix)\n",
    "    B_scaled = np.dot(scale_matrix,B)\n",
    "    \n",
    "    bern = np.random.binomial(1, prob_dist)\n",
    "    \n",
    "    sample_indices = np.where(bern == 1)[0]\n",
    "    \n",
    "    # sample k column-row pairs to form new smaller matrices\n",
    "    A_k_cols = A_scaled[:,sample_indices]\n",
    "    B_k_rows = B_scaled[sample_indices,:]\n",
    "       \n",
    "    # multiply smaller matrices\n",
    "    C_approx = np.dot(A_k_cols, B_k_rows)\n",
    "  \n",
    "    if debug == True:\n",
    "        print ('a_col_norms is ' + str(a_col_norms))\n",
    "        print ('b_row_norms is ' + str(b_row_norms))\n",
    "        print ('norm_mult is ' + str(norm_mult))\n",
    "        print ('sum_norm_mult is ' + str(sum_norm_mult))\n",
    "        print ('prob_dist is ' + str(prob_dist))\n",
    "        print ('scale matrix is ' + str(scale_matrix))\n",
    "        print ('A_scaled is ' + str(A_scaled))\n",
    "        print ('B_scaled is ' + str(B_scaled))\n",
    "        print ('sample_indices are ' + str(sample_indices))\n",
    "        print ('num sampled indices is ' + str(len(sample_indices)))\n",
    "        print ('Frobenius error bound is '+ str(sum_norm_mult**2/K - np.sum(np.multiply(np.multiply(a_col_norms,a_col_norms),np.multiply(b_row_norms,b_row_norms)))))\n",
    "        print ('A_k_cols is ')\n",
    "        print (A_k_cols)\n",
    "        print ('B_k_rows is ')\n",
    "        print (B_k_rows)\n",
    "        print ('C_approx is ')\n",
    "        print (C_approx)\n",
    "    \n",
    "    return C_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximation algorithm - random walks (Cohen and Lewis, 1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A,B - input matrices\n",
    "    S - number of samples (random walks per row)\n",
    "    negative_entries - True means entries can be negative, False means not\n",
    "'''\n",
    "def random_walks(A,B,S,negative_entries = False, debug=False):\n",
    "    walk_trace = np.zeros((A.shape[0],B.shape[1]))\n",
    "    \n",
    "    if negative_entries == False:\n",
    "        # compute W for layer 2 nodes\n",
    "        W2 = np.sum(B,axis=1)\n",
    "\n",
    "        # compute W for layer  1 nodes\n",
    "        W1 = np.dot(A,W2.T)\n",
    "\n",
    "        # assign probabilities for layer 2\n",
    "        P2_denominator = np.diag(1/W2)\n",
    "        P2 = np.dot(P2_denominator, B)\n",
    "\n",
    "        # assign probabilities for layer 1\n",
    "        P1_nominator = np.dot(A, np.diag(W2))\n",
    "        P1_denominator = np.diag(1/W1)\n",
    "        P1 = np.dot(P1_denominator,P1_nominator)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Scoring\n",
    "    # loop over rows\n",
    "    for node0 in range(A.shape[0]):\n",
    "        # take S samples for each row\n",
    "        for s in range(S):\n",
    "            # travel from node0 to node1 according to probability distribution of row node0 in P1\n",
    "            node1 = np.random.choice(P1.shape[1], size=1, p=P1[node0,:])[0]\n",
    "\n",
    "            # travel from node1 to node2 according to probability distribution of row node1 in P2\n",
    "            node2 = np.random.choice(P2.shape[1], size=1, p=P2[node1,:])[0]\n",
    "            \n",
    "            # update trace\n",
    "            walk_trace[node0][node2] += 1\n",
    "    \n",
    "    # estimate product from trace\n",
    "    C_approx = np.dot(np.diag(W1),walk_trace)/S\n",
    "    return C_approx\n",
    "    \n",
    "    \n",
    "    if debug == True:\n",
    "        print ('W2 is ' + str(W2))\n",
    "        print ('W1 is ' + str(W1))\n",
    "        print ('P2_denominator is ' + str(P2_denominator))\n",
    "        print ('P2 is ' + str(P2))\n",
    "        print ('P1_denominator is ' + str(P1_denominator))\n",
    "        print ('P1 is ' + str(P1))\n",
    "        print ('C_approx is ' + str(C_approx))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximation algorithm - SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A,B - input matrices\n",
    "    K - number of singular vectors to take from each matrix\n",
    "    this function performs full SVD decomposition, which is more computationaly expensive than\n",
    "    matrix multiplication itself. We will use it as a measure of maximal accuracy for other approximation algorithms \n",
    "'''\n",
    "def svd_mul(A,B,K,debug=False):\n",
    "    A_U, A_s, A_V = np.linalg.svd(A)\n",
    "    B_U, B_s, B_V = np.linalg.svd(B)\n",
    "    A_U_k = A_U[:,:K]\n",
    "    A_s_k = A_s[:K]\n",
    "    A_V_k = A_V[:K,:]\n",
    "    B_U_k = B_U[:,:K]\n",
    "    B_s_k = B_s[:K]\n",
    "    B_V_k = B_V[:K,:]   \n",
    "    \n",
    "    A_s_k_A_V_k = np.dot(np.diag(A_s_k),A_V_k)\n",
    "    B_U_k_B_s_k = np.dot(B_U_k, np.diag(B_s_k))\n",
    "    A_s_k_A_V_k_B_U_k_B_s_k = np.dot(A_s_k_A_V_k, B_U_k_B_s_k)\n",
    "    \n",
    "    C_approx = np.dot(np.dot(A_U_k, A_s_k_A_V_k_B_U_k_B_s_k), B_V_k)\n",
    "    return C_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to display approximation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(A,B,approx):\n",
    "    stats = {}\n",
    "    \n",
    "    # Calculate accurate multiplication result C=AB\n",
    "    acc = np.dot(A,B)\n",
    "    \n",
    "    # Relative error of Frobenius norm   \n",
    "    relative_frobenius_error = np.abs(np.linalg.norm(acc, ord='fro')-np.linalg.norm(approx, ord='fro'))/np.linalg.norm(acc, ord='fro')\n",
    "\n",
    "    # Relative error of spectal norm\n",
    "    relative_spectral_error = np.abs(np.linalg.norm(acc, ord=2)-np.linalg.norm(approx, ord=2))/np.linalg.norm(acc, ord=2)\n",
    "      \n",
    "    # Frobenius norm of the error matrix (acc-approx)\n",
    "    error_matrix_frobenius = np.linalg.norm(acc-approx, ord='fro')\n",
    "    \n",
    "    # Normalized Frobenius error F(acc-approx)/(F(A)F(B))\n",
    "    normalized_frobenius_error= np.linalg.norm(acc-approx, ord='fro')/(np.linalg.norm(A, ord='fro')*np.linalg.norm(B, ord='fro'))\n",
    "    \n",
    "    # Spectral norm of the error matrix\n",
    "    error_matrix_spectral = np.linalg.norm(acc-approx, ord=2)\n",
    "        \n",
    "    # Average per-element error:\n",
    "    per_element_error_avg = np.mean(np.abs(acc-approx))\n",
    "        \n",
    "    # Std of the per-element error:\n",
    "    per_element_error_std = np.std(acc-approx)\n",
    "    \n",
    "    stats['relative_frobenius_error'] = relative_frobenius_error\n",
    "    stats['relative_spectral_error'] = relative_spectral_error\n",
    "    stats['error_matrix_frobenius'] = error_matrix_frobenius\n",
    "    stats['normalized_frobenius_error'] = normalized_frobenius_error\n",
    "    stats['error_matrix_spectral'] = error_matrix_spectral\n",
    "    stats['per_element_error_avg'] = per_element_error_avg\n",
    "    stats['per_element_error_std'] = per_element_error_std\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats, name):\n",
    "\n",
    "    print ('Statistics for approximation method ' + name)\n",
    "    print ('Relative Frobenius error is: ' + str(stats['relative_frobenius_error']))\n",
    "    print ('Relative Spectral error is: ' + str(stats['relative_spectral_error']))\n",
    "    print ('Frobenius norm of error matrix: ' + str(stats['error_matrix_frobenius']))\n",
    "    print ('Frobenius norm of error matrix normalized: ' + str(stats['normalized_frobenius_error']))\n",
    "    print ('Spectral norm of error matrix: ' + str(stats['error_matrix_spectral']))\n",
    "    print ('Average per-element error: ' + str(stats['per_element_error_avg']))\n",
    "    print ('Standard deviation of the per-element error: ' + str(stats['per_element_error_std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First experiment - 100x100 matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k is 1\n",
      "k is 2\n",
      "k is 3\n",
      "k is 4\n",
      "k is 5\n",
      "k is 6\n",
      "k is 7\n",
      "k is 8\n",
      "k is 9\n",
      "k is 10\n",
      "k is 11\n",
      "k is 12\n",
      "k is 13\n",
      "k is 14\n",
      "k is 15\n",
      "k is 16\n",
      "k is 17\n",
      "k is 18\n",
      "k is 19\n",
      "k is 20\n",
      "k is 21\n",
      "k is 22\n",
      "k is 23\n",
      "k is 24\n",
      "k is 25\n",
      "k is 26\n",
      "k is 27\n",
      "k is 28\n",
      "k is 29\n",
      "k is 30\n",
      "k is 31\n",
      "k is 32\n",
      "k is 33\n",
      "k is 34\n",
      "k is 35\n",
      "k is 36\n",
      "k is 37\n",
      "k is 38\n",
      "k is 39\n",
      "k is 40\n",
      "k is 41\n",
      "k is 42\n",
      "k is 43\n",
      "k is 44\n",
      "k is 45\n",
      "k is 46\n",
      "k is 47\n",
      "k is 48\n",
      "k is 49\n",
      "k is 50\n",
      "k is 51\n",
      "k is 52\n",
      "k is 53\n",
      "k is 54\n",
      "k is 55\n",
      "k is 56\n",
      "k is 57\n",
      "k is 58\n",
      "k is 59\n",
      "k is 60\n",
      "k is 61\n",
      "k is 62\n",
      "k is 63\n",
      "k is 64\n",
      "k is 65\n",
      "k is 66\n",
      "k is 67\n",
      "k is 68\n",
      "k is 69\n",
      "k is 70\n",
      "k is 71\n",
      "k is 72\n",
      "k is 73\n",
      "k is 74\n",
      "k is 75\n",
      "k is 76\n",
      "k is 77\n",
      "k is 78\n",
      "k is 79\n",
      "k is 80\n",
      "k is 81\n",
      "k is 82\n",
      "k is 83\n",
      "k is 84\n",
      "k is 85\n",
      "k is 86\n",
      "k is 87\n",
      "k is 88\n",
      "k is 89\n",
      "k is 90\n",
      "k is 91\n",
      "k is 92\n",
      "k is 93\n",
      "k is 94\n",
      "k is 95\n",
      "k is 96\n",
      "k is 97\n",
      "k is 98\n",
      "k is 99\n",
      "k is 100\n"
     ]
    }
   ],
   "source": [
    "# initialize 100x100 random matrices (~N(0,1)\n",
    "M = 100\n",
    "K = 100\n",
    "N = 100\n",
    "\n",
    "# Number of experiments per data point to average on\n",
    "repeat_experiments = 1000\n",
    "\n",
    "algorithms = ['top_k', \n",
    "              #'colrow_uni_rep',\n",
    "              'colrow_opt_rep',\n",
    "              #'colrow_uni_norep',\n",
    "              #'colrow_opt_norep',\n",
    "              #'colrow_uni_rep_noscale',\n",
    "              #'colrow_opt_rep_noscale',\n",
    "              #'colrow_uni_norep_noscale',\n",
    "              #'colrow_opt_norep_noscale',\n",
    "              'colrow_bern_scale']#,\n",
    "              #'colrow_bern_noscale']\n",
    "              #'svd_mul']\n",
    "\n",
    "stat_names = ['relative_frobenius_error',\n",
    "              'relative_spectral_error',\n",
    "              'error_matrix_frobenius',\n",
    "              'normalized_frobenius_error',\n",
    "              'error_matrix_spectral',\n",
    "              'per_element_error_avg',\n",
    "              'per_element_error_std']\n",
    "\n",
    "writers = {}\n",
    "files = []\n",
    "C_approx = {}\n",
    "\n",
    "# Create one output csv file per stat\n",
    "for i in range(len(stat_names)):\n",
    "    files.append(open(stat_names[i] + '.csv', 'w', newline=''))\n",
    "    writers[stat_names[i]] = csv.writer(files[i])\n",
    "    writers[stat_names[i]].writerow([stat_names[i]])\n",
    "    if stat_names[i] != 'normalized_frobenius_error':\n",
    "        writers[stat_names[i]].writerow(['k'] + algorithms)\n",
    "    else:\n",
    "        writers[stat_names[i]].writerow(['k'] + algorithms + ['normalized frobenius error upper bound'])\n",
    "\n",
    "for k in range(1,K+1,1):    \n",
    "    print ('k is ' + str(k))\n",
    "    C_stats_sum = {}\n",
    "    for i in range(len(stat_names)):\n",
    "        C_stats_sum[stat_names[i]] = {}\n",
    "        for j in range(len(algorithms)):\n",
    "            C_stats_sum[stat_names[i]][algorithms[j]] = 0.0\n",
    "    \n",
    "    for n in range(repeat_experiments):       \n",
    "    \n",
    "        # Sample input matrices\n",
    "        #A = np.random.normal(loc=np.random.randint(-1,1), scale=1, size=(M,K))\n",
    "        #A = np.zeros([M,K])\n",
    "        #A = np.random.zipf(a=1e8, size=(M,K))\n",
    "        A = np.random.normal(loc=1, scale=1, size=(M,K))\n",
    "        #B = np.random.normal(loc=np.random.randint(-1,1), scale=1, size=(K,N))\n",
    "        B = np.random.normal(loc=1, scale=1, size=(K,N))\n",
    "        #B = np.zeros([K,N])\n",
    "        #for i in range(K):\n",
    "        #    l = np.random.randint(-(i+1),i+1)\n",
    "        #    B[i,:] = np.random.normal(loc=l, scale=1, size=(1,N))\n",
    "        #    A[:,i] = np.random.normal(loc=l, scale=1, size=(1,M))\n",
    "        \n",
    "        #B = np.random.zipf(a=1e8, size=(K,N))\n",
    "        #B = np.random.normal(loc=1, scale=1, size=(K,N))\n",
    "\n",
    "        \n",
    "        \n",
    "        # Calculate the product AB using different approximation algorithms\n",
    "        C_approx['top_k'] = top_k(A,B,k)\n",
    "        #C_approx['colrow_uni_rep'] = column_row(A,B,k,with_replacement=True, optimal_prob=False, scale=True)\n",
    "        C_approx['colrow_opt_rep'] = column_row(A,B,k,with_replacement=True, optimal_prob=True, scale=True)\n",
    "        #C_approx['colrow_uni_norep'] = column_row(A,B,k,with_replacement=False, optimal_prob=False, scale=True)\n",
    "        #C_approx['colrow_opt_norep'] = column_row(A,B,k,with_replacement=False, optimal_prob=True, scale=True)\n",
    "        #C_approx['colrow_uni_rep_noscale'] = column_row(A,B,k,with_replacement=True, optimal_prob=False, scale=False)\n",
    "        #C_approx['colrow_opt_rep_noscale'] = column_row(A,B,k,with_replacement=True, optimal_prob=True, scale=False)\n",
    "        #C_approx['colrow_uni_norep_noscale'] = column_row(A,B,k,with_replacement=False, optimal_prob=False, scale=False)\n",
    "        #C_approx['colrow_opt_norep_noscale'] = column_row(A,B,k,with_replacement=False, optimal_prob=True, scale=False)\n",
    "        C_approx['colrow_bern_scale'] = column_row_bern(A,B,k, scale=True)\n",
    "        #C_approx['colrow_bern_noscale'] = column_row_bern(A,B,k, scale=False)\n",
    "        #C_approx['svd_mul'] = svd_mul(A,B,k)\n",
    "        \n",
    "        # Get statistics for each algorithm\n",
    "        C_stats_current = {}\n",
    "        for j in range(len(algorithms)):\n",
    "            C_stats_current[algorithms[j]] = get_stats(A,B, C_approx[algorithms[j]])\n",
    "            for i in range(len(stat_names)):\n",
    "                C_stats_sum[stat_names[i]][algorithms[j]] += C_stats_current[algorithms[j]][stat_names[i]]\n",
    "        \n",
    "    \n",
    "    # Output format:\n",
    "    # a separate csv per statistic\n",
    "    # for each csv - \n",
    "    #     the columns are the same statistic for different algorithms\n",
    "    #     the rows are different values of k sampled \n",
    "    for i in range(len(stat_names)):\n",
    "        stats = [k]\n",
    "        for j in range(len(algorithms)): \n",
    "            stats.append(C_stats_sum[stat_names[i]][algorithms[j]]/repeat_experiments)\n",
    "        if stat_names[i] != 'normalized_frobenius_error':\n",
    "            writers[stat_names[i]].writerow(stats)\n",
    "        else:\n",
    "            writers[stat_names[i]].writerow(stats + [1/np.sqrt(k)])\n",
    "\n",
    "    \n",
    "# Close stat files\n",
    "for i in range(len(stat_names)):\n",
    "    files[i].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([[1,2,3],[4,5,6]])\n",
    "E = np.array([[7,8],[9,10],[11,12]])\n",
    "k=2\n",
    "print D\n",
    "print E\n",
    "print np.dot(D,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  56.30769231,   65.69230769],\n",
       "       [ 112.69230769,  180.30769231]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_walks(D,E,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  58.17058972,   64.43900704],\n",
       "       [ 138.88714117,  153.85351103]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_mul(D,E,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_col_norms is [2.11667059 1.94337924 1.82620943 2.35940971 1.48550002]\n",
      "b_row_norms is [1.35103635 2.16050556 2.36527216 3.28954819 1.16040812]\n",
      "norm_mult is [2.8596989  4.19868166 4.31948233 7.76139195 1.72378629]\n",
      "sum_norm_mult is 20.863041116407594\n",
      "prob_dist is [0.27414018 0.40249949 0.41407984 0.74403266 0.16524784]\n",
      "scale matrix is [[1.90991331 0.         0.         0.         0.        ]\n",
      " [0.         1.57622182 0.         0.         0.        ]\n",
      " [0.         0.         1.55402485 0.         0.        ]\n",
      " [0.         0.         0.         1.1593218  0.        ]\n",
      " [0.         0.         0.         0.         2.45998296]]\n",
      "A_scaled is [[-0.43341211  2.76578687 -0.7452519  -0.90286895  1.97687315]\n",
      " [-2.30714529  0.76485705  0.06841987 -2.49133003 -0.91792323]\n",
      " [ 0.612707   -0.98997047  0.81475097 -0.574065    1.54083665]\n",
      " [ 1.12231855  0.18524605  2.60254433 -0.02447616 -2.29819639]\n",
      " [ 3.03270574 -0.36637881  0.23866989 -0.36041828  0.97336954]]\n",
      "B_scaled is [[-2.06867715  0.32470341 -0.21605912 -1.4822264  -0.17243984]\n",
      " [-0.74980325 -1.24582809  1.27396242 -2.73638704 -0.60984232]\n",
      " [ 2.06341377 -0.17544206  0.37407049  1.62244737  2.53968287]\n",
      " [ 0.59609741  0.87782198 -3.21534427  1.0082788  -1.43628658]\n",
      " [ 1.66691448 -0.60461066 -0.99416146 -0.1100668   2.00100509]]\n",
      "sample_indices are [1 3]\n",
      "num sampled indices is 2\n",
      "Frobenius error bound is 109.95786518175358\n",
      "A_k_cols is \n",
      "[[ 2.76578687 -0.90286895]\n",
      " [ 0.76485705 -2.49133003]\n",
      " [-0.98997047 -0.574065  ]\n",
      " [ 0.18524605 -0.02447616]\n",
      " [-0.36637881 -0.36041828]]\n",
      "B_k_rows is \n",
      "[[-0.74980325 -1.24582809  1.27396242 -2.73638704 -0.60984232]\n",
      " [ 0.59609741  0.87782198 -3.21534427  1.0082788  -1.43628658]]\n",
      "C_approx is \n",
      "[[-2.61199383 -4.23825317  6.42654302 -8.47860696 -0.38991532]\n",
      " [-2.05856769 -3.13982464  8.98488288 -4.60490017  3.1118217 ]\n",
      " [ 0.40008442  0.72940614  0.58463144  2.13012479  1.42824774]\n",
      " [-0.15348827 -0.25227044  0.31469579 -0.53158368 -0.0778161 ]\n",
      " [ 0.05986762  0.14006193  0.69211601  0.63915212  0.74109724]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.53042751, -2.07972341,  3.26291153, -4.39453807, -0.56630209],\n",
       "       [-1.83680571, -1.0105038 ,  2.33457578, -2.89365602,  0.75972393],\n",
       "       [-0.417174  ,  0.76652379, -0.11786664,  1.20005904, -0.76557205],\n",
       "       [-1.04086437, -0.28384309, -0.55304795, -1.64340119, -1.98243847],\n",
       "       [ 1.35697524,  0.03637697,  0.32031548,  1.59567068, -0.16347814]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.normal(loc=0, scale=1, size=(5,5))\n",
    "B = np.random.normal(loc=0, scale=1, size=(5,5))\n",
    "column_row_bern(A,B,2, scale=True, debug=True)-A.dot(B)\n",
    "\n",
    "\n",
    "#random_walks(A,B,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.12353307,   2.70736565,   0.08641966, ...,   1.23592005,\n",
       "         -6.78048297,   0.56506958],\n",
       "       [  2.71544266,   0.08661535,   0.12344578, ...,  -8.42435018,\n",
       "          4.88709719,  -0.16644529],\n",
       "       [ -9.82749755,   9.39555596,  14.12118491, ...,  -4.63854117,\n",
       "         -9.72714385,  10.20529936],\n",
       "       ..., \n",
       "       [  0.39364931,   5.22158707,  -0.96962055, ..., -10.40712015,\n",
       "          9.66697407,  -0.64169166],\n",
       "       [  3.36894029,  -4.58071512,  -3.42604357, ...,  -1.31869436,\n",
       "          5.15108826,  -2.62539348],\n",
       "       [  1.93828184,  -0.44931835,  -1.28170357, ...,  -8.33528075,\n",
       "         10.81260351,  -0.60708691]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_mul(A,B,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.43568934,  -3.90850717,  -4.50532006, ..., -12.67101179,\n",
       "        -10.15682965, -13.75763405],\n",
       "       [  9.7159455 ,   5.77210303,  -6.70278586, ..., -14.93998827,\n",
       "         10.07291841,  -9.7994708 ],\n",
       "       [  5.50987195,  15.09380985,  21.62866239, ...,  -6.55928461,\n",
       "        -11.92391737,   4.81192288],\n",
       "       ..., \n",
       "       [-14.33451067,  -5.04033338,  11.80525566, ..., -12.38111982,\n",
       "         -6.37253943,   4.20435141],\n",
       "       [ -2.22791811,   0.33741418,   3.82464388, ...,  -0.50160204,\n",
       "         26.72042787, -11.79607345],\n",
       "       [  2.04571995,   1.14928371,  -0.52280088, ..., -10.55945816,\n",
       "         -2.95092781,  13.93082587]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.83325374,  1.81662122,  1.10048855, ..., -3.89241902,\n",
       "         1.0590615 , -0.80584154],\n",
       "       [-1.45229436,  4.47517318, -2.77303134, ...,  0.54945197,\n",
       "         2.17507427, -1.27126498],\n",
       "       [ 1.93681846,  4.02029345,  2.38006873, ..., -3.93959194,\n",
       "        -1.10964247, -4.93127999],\n",
       "       ..., \n",
       "       [ 3.54270866, -8.78806713, -3.66210396, ...,  5.23597104,\n",
       "         0.55799719, -0.15416489],\n",
       "       [-6.07363412,  5.35158953,  0.55807345, ...,  1.25972357,\n",
       "        -0.28438256, -5.133833  ],\n",
       "       [ 9.08320794, -4.20921349, -2.06990813, ..., -0.70497197,\n",
       "        -0.91626798,  5.23245363]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k(A,B,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[-0.56948143,0.6234533,0.39866474,0.21468586,-0.8736436 ],\n",
    " [ 0.83005655,  0.09830075, -0.03885731, -0.2037122,   1.9092574, ],\n",
    " [-1.7054096,  -0.10213567,  0.15305512, -1.164082,   -1.1722519 ],\n",
    " [-0.4288956,   1.1507404,  -1.7422528,  -0.02039167, -0.00957074],\n",
    " [ 1.8480316,  -0.8813653,  -0.07361737,  0.85047555,  0.43315265]]\n",
    ")\n",
    "B = np.array([[ 0.2528034,   0.85932934,  0.7415126,  -0.7723672,   1.1472509 ],\n",
    " [-0.7447469,  -0.50370145,  0.18461666,  0.16899914,  0.93716013],\n",
    " [-0.5356013,  -1.5470285,  -2.0807953,   0.4024434,   0.8996739 ],\n",
    " [-1.7614233,  -1.2215439,  -0.75340575, -0.23610812, -1.4844245 ],\n",
    " [ 0.37762547, -0.40104532, -0.31718078,  0.71741456, -0.9783565 ]])\n",
    "A = np.random.normal(size=(50,50))\n",
    "B = np.random.zipf(a=2, size=(50,50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  11.08711404  -54.09708129   35.04564521 ...,  -64.69599955\n",
      "   -30.18062722  -32.83347395]\n",
      " [  -3.65045329  275.15997442    7.58957963 ..., -175.13654913\n",
      "    14.61493058  -12.96219937]\n",
      " [ -21.81520671  409.07492208  -36.71183062 ...,    1.10830785\n",
      "   -30.60327048  -21.77099869]\n",
      " ..., \n",
      " [  20.6222693   -17.56464522   34.83065905 ...,  126.64939202   17.6626778\n",
      "    13.4089341 ]\n",
      " [-117.1839508    -8.82787764  -15.5182248  ...,  -31.30414425\n",
      "     2.61513924   20.00294063]\n",
      " [ -60.88094067 -201.32990652  -36.91815021 ...,  -54.50376384\n",
      "    16.67376514  -15.59686276]]\n"
     ]
    }
   ],
   "source": [
    "print(A.dot(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  47.38387061  -15.67847164  -15.28636685 ...,    3.00901655\n",
      "    -1.14483605    9.06508816]\n",
      " [-154.36400622  -39.82783016  -35.14531024 ...,  -32.93814493\n",
      "   -31.91303924  -54.88639593]\n",
      " [ -10.7142406    10.16820778   -5.87699611 ...,   -0.89513589\n",
      "    -0.60178752   -3.7310086 ]\n",
      " ..., \n",
      " [  24.44056688   46.8047143    17.79101094 ...,    9.90229623\n",
      "    12.96267789   18.77174239]\n",
      " [  26.61743539  -45.33496999  -19.80077321 ...,    1.59527248\n",
      "    -5.33122068    4.11783945]\n",
      " [ -63.45978797  -79.972869    -30.41461971 ...,  -13.13251341\n",
      "   -19.02233942  -17.72792278]]\n"
     ]
    }
   ],
   "source": [
    "print(column_row(A,B,25,with_replacement=True, optimal_prob=True, scale=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89857.6214422\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(column_row(A,B,5,with_replacement=False, optimal_prob=True, scale=True)-A.dot(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.03527263e+01  -3.41643706e+01   1.21056433e+01 ...,   6.20017050e+01\n",
      "   -7.85729402e+00  -1.51572029e+01]\n",
      " [  1.90507469e+00   2.78138373e+02   1.74872795e-01 ...,  -6.36404616e+01\n",
      "    6.26443194e+00  -1.73418464e+01]\n",
      " [ -1.85158693e+01   4.36089868e+02  -3.36040592e+01 ...,   4.66281294e+01\n",
      "    1.10722363e+01  -5.07384593e+00]\n",
      " ..., \n",
      " [  1.61779260e+01  -3.30297336e+01   3.74283192e+01 ...,  -1.60047771e+01\n",
      "    8.41049612e+00   1.43385627e+01]\n",
      " [ -1.18756131e+02  -1.13690487e+01  -4.40885771e+01 ...,  -5.13886032e+00\n",
      "   -7.21229172e+00  -1.70976270e+01]\n",
      " [ -4.97545295e+01  -2.15840289e+02  -1.54925629e+01 ...,   2.68197579e+01\n",
      "   -2.11246927e+01  -1.21211718e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(top_k(A,B,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
