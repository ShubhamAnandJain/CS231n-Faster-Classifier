{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1653204886178,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "bFB6WDmimTpS",
    "outputId": "672d2a42-ea4c-43f3-e1ca-1367c2868357"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# FOLDERNAME = 'CS231n_project/'\n",
    "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4353,
     "status": "ok",
     "timestamp": 1653204890527,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "ejjKJYC2YffB",
    "outputId": "7a1aad97-7fc6-4c21-fa04-24f4508284e0"
   },
   "outputs": [],
   "source": [
    "# !pip install torch==1.7 torchvision==0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6003,
     "status": "ok",
     "timestamp": 1653204896527,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "TzJV-LLVhBt5",
    "outputId": "03468936-f099-4c8f-935b-9dfc257e456a"
   },
   "outputs": [],
   "source": [
    "# %cd approx/src/pytorch/cpp\n",
    "# !python setup.py install\n",
    "# %cd ../../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1653204897473,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "uGvmWqNZD2hh",
    "outputId": "fd0ce3ef-78c9-4a72-bef1-a3b94a03a0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from conv_norm import PreConv\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from utils import ImportanceSampler\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1653204897475,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "BMatykiuGnJT"
   },
   "outputs": [],
   "source": [
    "from utils import get_accuracy, load_dataset\n",
    "from models import get_model\n",
    "check_accuracy = lambda loader, model: get_accuracy(loader, model, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1653206021690,
     "user": {
      "displayName": "Shubham Jain",
      "userId": "09959993659061500830"
     },
     "user_tz": 420
    },
    "id": "dLHtcW9UHqOt"
   },
   "outputs": [],
   "source": [
    "def train_model(model_name, dataset_name, model_params={}, hyperparams={}):\n",
    "\n",
    "  learning_rate = hyperparams.get('lr', 1e-3)\n",
    "  num_epochs = hyperparams.get('num_epochs', 10)\n",
    "  weight_decay = hyperparams.get('weight_decay', 0)\n",
    "  train_ratio = hyperparams.get('train_ratio', 0.8)\n",
    "  batch_size = hyperparams.get('batch_size', 64)\n",
    "  seed = hyperparams.get('seed', 0)\n",
    "  imp_sampling = model_params.get('importance_sampling', False)\n",
    "  gamma = model_params.get('gamma', 0.9)\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  loader_train, loader_val, loader_test, num_train, num_channels = load_dataset(dataset_name, train_ratio, batch_size)\n",
    "  model = get_model(model_name, model_params, learning_rate, loader_train, num_channels, device)\n",
    "\n",
    "  print(\"Model architecture:\")\n",
    "  print(model)\n",
    "\n",
    "  print(f'INFO: Training {model_name} on {dataset_name} with lr {learning_rate}, num_epochs={num_epochs}, weight_decay={weight_decay}')\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "\n",
    "  epoch_vals = []\n",
    "  \n",
    "  weight = torch.tensor([1.0]*num_train)\n",
    "\n",
    "  t_acc, t_loss = check_accuracy(loader_train, model)\n",
    "  val_acc, val_loss = check_accuracy(loader_val, model)\n",
    "  \n",
    "  start = timer()\n",
    "  c_time = timer()-start\n",
    "\n",
    "  print(f'Plot: Train, {0}, {t_loss:.3f}, {t_acc:.2f}, {c_time:.1f}')\n",
    "  print(f'Plot: Val, {0}, {val_loss:.3f}, {val_acc:.2f}, {c_time:.1f}')\n",
    "\n",
    "  for e in range(num_epochs):\n",
    "    model.train()\n",
    "    doUniform = (e == 0) or (imp_sampling == False)\n",
    "    loader_train_sampled = loader_train\n",
    "    if not doUniform:\n",
    "      train_sampler = ImportanceSampler(num_train, weight, batch_size)\n",
    "      loader_train_sampled, _, _, _, _ = load_dataset(dataset_name, train_ratio, batch_size, train_sampler)\n",
    "    \n",
    "    for t, tpl in enumerate(loader_train_sampled):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()  # put model to training mode\n",
    "        x = tpl[0].to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        y = tpl[1].to(device=device, dtype=torch.long)\n",
    "\n",
    "        scores = model(x)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not doUniform:\n",
    "          idx = tpl[2]\n",
    "          weight[idx] = gamma * weight[idx] + (1 - gamma) * float(loss)\n",
    "\n",
    "    t_acc, t_loss = check_accuracy(loader_train, model)\n",
    "    model.eval()\n",
    "    val_acc, val_loss = check_accuracy(loader_val, model)\n",
    "    c_time = timer()-start\n",
    "\n",
    "    print(f'Plot: Train, {e+1}, {t_loss:.3f}, {t_acc:.2f}, {c_time:.1f}')\n",
    "    print(f'Plot: Val, {e+1}, {val_loss:.3f}, {val_acc:.2f}, {c_time:.1f}')\n",
    "\n",
    "  test_acc, test_loss = check_accuracy(loader_test, model)\n",
    "  print(f'Plot: Test, {val_loss:.3f}, {val_acc:.2f}, {c_time:.1f}')\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhJmW7E1cMBF",
    "outputId": "d812c321-f54b-4aaf-c69b-ce76893369c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CIFAR100 Train dataset raw mean: 0.4783550798892975, raw std dev: 0.2678655982017517\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "INFO: Size of dataset: Training 40000, Validation 10000, Test 10000\n",
      "bn_pres is True\n",
      "Originally num layers were 20\n",
      "Replace layers list is: [0]\n",
      "Number of og layers are 20, number of new layers are 1\n",
      "bn_pres is True\n",
      "Originally num layers were 20\n",
      "Replace layers list is: [0]\n",
      "Number of og layers are 19, number of new layers are 1\n",
      "Model architecture:\n",
      "ResNet(\n",
      "  (conv1): PreConv(\n",
      "    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n",
      "    (bpconv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "INFO: Training Resnet18 on CIFAR100 with lr 0.003, num_epochs=25, weight_decay=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/CS231n-Faster-Classifier/src/conv_norm.py:112: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  f_input = torch.rfft(cout, 2, normalized=False, onesided=True)\n",
      "/home/ubuntu/CS231n-Faster-Classifier/src/conv_norm.py:123: UserWarning: The function torch.irfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.ifft or torch.fft.irfft. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/aten/src/ATen/native/SpectralOps.cpp:602.)\n",
      "  output = torch.irfft(compl_mul_2D(f_input, V), 2, normalized=False, signal_sizes=(final_size_x,final_size_y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot: Train, 0, 6.908, 0.00, 0.0\n",
      "Plot: Val, 0, 6.907, 0.00, 0.0\n",
      "Plot: Train, 1, 3.785, 10.80, 112.2\n",
      "Plot: Val, 1, 4.088, 9.56, 112.2\n",
      "Plot: Train, 2, 3.359, 17.44, 225.6\n",
      "Plot: Val, 2, 3.801, 15.54, 225.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2139/2208795676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resnet18'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CIFAR100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtest_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2139/2208795676.py\u001b[0m in \u001b[0;36mtest_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'convnorm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode_conv\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmodel_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'convnorm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode_bn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resnet18'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CIFAR100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mtest_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2139/633822996.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name, dataset_name, model_params, hyperparams)\u001b[0m\n\u001b[1;32m     61\u001b[0m           \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mt_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2139/3748356230.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheck_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CS231n-Faster-Classifier/src/utils.py\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(loader, model, device, dtype)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# move to device, e.g. GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# learning_rate = hyperparams.get('lr', 1e-3)\n",
    "# num_epochs = hyperparams.get('num_epochs', 10)\n",
    "# weight_decay = hyperparams.get('weight_decay', 0)\n",
    "# train_ratio = hyperparams.get('train_ratio', 0.8)\n",
    "# batch_size = hyperparams.get('batch_size', 64)\n",
    "# imp_sampling = model_params.get('importance_sampling', False)\n",
    "# gamma = model_params.get('gamma', 0.9)\n",
    "\n",
    "# gradinit_params = {\n",
    "#     \"gradinit_iters\": 50,\n",
    "# }\n",
    "model_params = {\n",
    "    # \"gradinit\": gradinit_params,\n",
    "    \"convnorm\" : {\"mode_conv\": [('first_frac', 0.25)], \"mode_bn\": [('first_frac', 0.25)]},\n",
    "    # \"approx_mult\" : 0.2,\n",
    "    # \"importance_sampling\" : True,\n",
    "    # \"gamma\" : 0.9\n",
    "}\n",
    "hyperparams = {\n",
    "    \"lr\" : 3e-3,\n",
    "    \"num_epochs\" : 25,\n",
    "    \"weight_decay\" : 0,\n",
    "    \"train_ratio\" : 0.8,\n",
    "    \"batch_size\" : 1024,\n",
    "}\n",
    "\n",
    "# train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
    "# model_params[\"convnorm\"] = True\n",
    "# train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
    "\n",
    "def test_setup():\n",
    "    modeList = [[('first_num', 1)], [('first_frac', 0.25)], [('first_frac', 0.5)], [('first_frac', 0.75)], [('first_frac', 1.0)]]\n",
    "    for mode in modeList:\n",
    "        model_params['convnorm'][\"mode_conv\"] = mode\n",
    "        model_params['convnorm'][\"mode_bn\"] = mode\n",
    "        train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
    "\n",
    "test_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_setup2():\n",
    "    modeList = [[('last_num', 1)], [('last_frac', 0.25)], [('last_frac', 0.5)], [('last_frac', 0.75)]]\n",
    "    for mode in modeList:\n",
    "        model_params['convnorm'][\"mode_conv\"] = mode\n",
    "        model_params['convnorm'][\"mode_bn\"] = mode\n",
    "        train_model('Resnet18', 'CIFAR100', model_params, hyperparams)\n",
    "\n",
    "test_setup2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_p37",
   "language": "python",
   "name": "pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
